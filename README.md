# CDS DS 121 - Foundations of Data Science II

Please note that all information provided below is current as of Spring 2023.

## Lecture Catalog

- Lecture 0: Course Overview
- Lecture 1: The Power of Linearity
- Lecture 2: Vectors and Vector Spaces
- Lecture 3: Linear (In)dependence
- Lecture 4: Solving Linear Equations with Gaussian Elimination
- Lecture 5: Gaussian Elimination (Continued)
- Lecture 6: Vector Geometry
- Lecture 7: The K-Means Problem
- Lecture 8: Cluster Evaluation
- Lecture 9: Hierarchical Clustering
- Lecture 10: Supervised Learning
- Lecture 11: Decision Trees
- Lecture 12: k-Nearest Neighbors
- Lecture 13: k-Nearest Neighbors in Action
- Lecture 14: Matrix Algebra
- Lecture 15: Computing with Real Numbers
- Lecture 16: Linear Transformations
  - 16.1 Matrices as functions
  - 16.2 Transformations
  - 16.3 Linear Transformations
  - 16.4 The Matrix of a Linear Transformation
  - 16.5 Rotations
- Lecture 17: Matrix Composition
  - 17.1 Visualizing Linear Transformations of R^2
  - 17.2 Existence and Uniqueness
  - 17.3 Area is Scaled by the Determinant
  - 17.4 Inverse of a matrix
  - 17.5 Finding the inverse of a matrix
- Lecture 18: Matrix Factorization
  - 18.1 Finding the inverse of a matrix
  - 18.2 Solving linear systems using matrix inverses
  - 18.3 Matrix Multiplication and Factorization
  - 18.4 Computational Complexity of Matrix Multiplication
  - 18.5 Computational Complexity of Matrix Inverse
- Lecture 19: The LU Factorization
  - 19.1 Computational Complexity of Matrix Inverse
  - 19.2 Matrix Factorizations
  - 19.3 The LU Factorization Problem
  - 19.4 Elementary Matrices
  - 19.5 The LU Factorization
- Lecture 20: LU Decomposition + Subspaces
  - 20.1 Using the LU Factorization
  - 20.2 Pivoting
  - 20.3 The Invertible Matrix Theorem
- Lecture 21: Vector Subspace and Basis
  - 21.1 Subspaces
  - 21.2 Two Important Subspaces
  - 21.3 A Basis for a Subspace
  - 21.4 Basis for Null Space
  - 21.5 The Dimension of a Subspace
  - 21.6 Basis and Dimension for Column Space
  - 21.7 The Rank Theorem
- Lecture 22: Orthogonality
  - 22.1 Coordinate Systems
  - 22.2 Orthogonal Sets
  - 22.3 Orthogonal Basis
- Lecture 23: QR Decomposition
  - 23.1 Orthogonal Basis
  - 23.2 Orthogonal Projection
  - 23.3 QR Decomposition
    - Gram-Schmidt Orthogonalization
    - QR Decomposition
  - 23.4 Orthonormal Sets
    - Orthonormal Matrices
    - Orthonormal Tranformations
    - When Orthonormal Matrices are Square
- Lecture 24: Least Squares
  - 24.1 The Power of Approximation
    - When an Inconsistent System is Better than a Consistent System
    - Finding a Good Approximate Solution
  - 24.2 The General Least-Squares Problem
    - Interpretation of the Least Squares Problem
  - 24.3 Solving the General Least Squares Problem
    - The Orthogonal Decomposition Theorem
    - The Best Approximation Theorem
  - 24.4 Projection Solves Least Squares
    - The Normal Equations
    - When the Normal Equations have Multiple Solutions
- Lecture 25: Intro to Linear Regression
- Lecture 26: Multiple Regression
- Lecture 27: Eigenvectors and Eigenvalues
  - 27.1 The value of eigenvalues
  - 27.2 Scaling
  - 27.3 Checking Eigenvectors and Eigenvalues
  - 27.4 Finding the Eigenspace Given λ
    - Complex Eigenspaces
    - Eigenvalues of a Triangular Matrix
    - Invertibility and Eigenvalues
  - 27.5 Finding Eigenvalues λ using Determinants
    - Determinants for Square Matrices
    - Invertibility
  - 27.6 The Characteristic Equation
    - Similarity
- Lecture 28: Diagonalisation
  - 28.1 Similarity
  - 28.2 Diagonalisation
    - Powers of a Diagonal Matrix
    - Extending to a General Matrix A
  - 28.3 Diagonalisation Requires Eigenvectors and Eigenvalues
  - 28.4 Diagonalizing a Matrix
    - Four Steps to Diagonalization
      - Step 1: Find the eigenvalues of A
      - Step 2: Find three linearly independent eigenvectors of A
      - Step 3: Construct P from the vectors in Step 2
      - Step 4: Construct D from the corresponding eigenvalues
    - When Diagonalization Fails
    - An Important Case
  - 28.5 Diagonalization as a Change of Basis
    - Interpreting Diagonalisation Geometrically
- Lecture 29: Symmetric Matrices
  - 29.1 Diagonalising a Symmetric Matrix
    - The Special Theorem
    - Orthogonal Diagonalisation
  - 29.2 Quadratic Forms
    - Visulizing Quadratic Forms
  - 29.3 Classifying Quadratic Forms
- Lecture 30: Constrained Optimisation
  - 30.1 Constrained Optimisation
  - 30.2 Solving Constrained Optimisation Over Quadratic Forms
    - Eigenvalues Solve Contrained Optimisation
    - Singular Value Decomposition is a constrained optimization problem
  - 30.3 Cholesky decomposition
- Lecture 31: Singular Value Decomposition
  - 31.1 Our Last Matrix Factorization
    - The data science view
    - The geometric view
    - The algebraic view
  - 31.2 Maximizing ∥Ax∥
    - ∥Ax∥^2 is a Quadratic Form
  - 31.3 The Singular Values of a Matrix
    - The Eigenvectors of AT A Lead To an Orthogonal Basis for Col A
- Lecture 32: Reduced SVD
  - 32.1 The Singular Value Decomposition
  - 32.2 Reduced SVD
  - 32.3 Dimensionality Reduction
  - 32.4 Empirical Examples of SVD
    - Image compression
    - Networks
    - Low Effective Rank is Common
    - Summary
- Lecture 33: SVD and Principal Component Analysis
  - 33.1 Empirical Examples of SVD
    - Image compression
    - Networks
    - Low Effective Rank is Common
      - User preferences over items
      - Likes on Facebook
      - Social Media Activity
  - 33.2 Interpretations of Low Effective Rank
    - Low Rank Implies Common Patterns
    - Low Rank Defines Latent Factors
- Lecture 34: Principal Component Analysis
  - 34.1 The Pseudoinverse
  - 34.2 Dimensionality Reduction and PCA
    - Centroid and Variance
  - 34.3 Using PCA for Visualization and Denoising
    - Our Dataset: 20 Newsgroups
    - Basic Clustering
    - Improvement: Stemming
    - Demonstrating PCA
    - Denoising
    - Visualization
- Lecture 35: Markov Chains
  - 35.1 Dynamical Systems
  - 35.2 Markov Chains
  - 35.3 Predicting the Distant Future
  - 35.4 Steady-State Vectors
  - 35.5 Finding the Steady State
  - 35.6 Existence of, and Convergence to, Steady State
- Lecture 36: Applications of Markov Chains
  - 36.1 Markov Chains and Eigenvectors
    - A complete solution for the evolution of a Markov Chain
  - 36.2 An early application of Markov Chains
- Lecture 37: PageRank
  - 37.1 The History of PageRank
    - How search engines work
    - The Insight
  - 37.2 Random Walks
  - 37.3 Absorbing Boundaries
  - 37.4 Random Walks on Undirected Graphs
  - 37.5 Random Walks on Directed Graphs
  - 37.6 PageRank
  - 37.7 Computing PageRank: The Power Method


## Instructors

- Professor: Allison McDonald
  - amcdon@bu.edu
- TA: Abhishek Tiwari
  - abhi6689@bu.edu

## Meetings

- Lecture:
  - Monday, Wednesday, Friday
  - 11:15 AM - 12:05 PM
  - PHO 211
- Recitation:
  - Monday at 1:25-2:15pm (KCB 103)
  - Monday at 2:30-3:20pm (FLR 121)
  - Monday at 3:35-4:25pm (FLR 121)

## Assignments & Test dates

- Mid-term Exam 1: Friday, February 24
- Mid-term Exam 2: Friday, March 31
- Final Exam: May 8 at 12-2pm
- Homeworks: Due on Fridays unless otherwise posted

## Websites

- [Piazza](https://piazza.com/bu/spring2023/ds121/info)
  - Access code: kmq40vhpgll
- [Gradescope](https://www.gradescope.com/courses/495963)
  - Entry code: 7G77JK
- [Zoom link for live lecture](https://bostonu.zoom.us/j/91752352732?pwd=NkdvVUNpV0FHUnB5TkZaQWZ0d2ZXUT09)